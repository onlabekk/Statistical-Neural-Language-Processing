{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "# 1. Information about the submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3",
      "metadata": {
        "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3"
      },
      "source": [
        "## 1.1 Name and number of the assignment "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9",
      "metadata": {
        "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9"
      },
      "source": [
        "Assignment 3: Taxonomy Enrichment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d",
      "metadata": {
        "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d"
      },
      "source": [
        "## 1.2 Student name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2",
      "metadata": {
        "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
      },
      "source": [
        "Kundyz Onlabek"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a46ab45-d215-41af-b910-63ff4a215a07",
      "metadata": {
        "id": "8a46ab45-d215-41af-b910-63ff4a215a07"
      },
      "source": [
        "## 1.3 Codalab user ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a",
      "metadata": {
        "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a"
      },
      "source": [
        "inflover"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af498ab-3c00-4d36-a962-c947862fede8",
      "metadata": {
        "id": "1af498ab-3c00-4d36-a962-c947862fede8"
      },
      "source": [
        "# 2. Technical Report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061f71b9-114a-4cb0-b531-5711970317bf",
      "metadata": {
        "id": "061f71b9-114a-4cb0-b531-5711970317bf"
      },
      "source": [
        "## 2.1 Methodology "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText is a library for learning of word embeddings and text classification created by Facebook's AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words."
      ],
      "metadata": {
        "id": "VZtYmRLd_92s"
      },
      "id": "VZtYmRLd_92s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KroU4N3H-tAW"
      },
      "source": [
        "Fasttext baseline:\n",
        "- Compute embeddings of all synsets in RuWordNet by averaging embeddings of all words from senses belonging to a synset.\n",
        "- Get embeddings for orphans. For multi-word orphans the embeddings are computed by averaging vectors for all words comprising an orphan.\n",
        "- For each orphan compute the top k = 10 closest synsets of the same part of speech as the orphan using the cosine similarity measure.\n",
        "- Extract hypernyms for each of these closest synsets from the previous step. Take the first n = 10 results (as each synset may have several hypernyms)."
      ],
      "id": "KroU4N3H-tAW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELMo (\"Embeddings from Language Model\") is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bi-directional LSTM which produces word-level embeddings. Like BERT (but unlike the word embeddings produced by \"Bag of Words\" approaches, and earlier vector approaches such as Word2Vec and GloVe), ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \"bank\" in \"river bank\" and \"bank balance\"."
      ],
      "metadata": {
        "id": "GtGUAEVN_vni"
      },
      "id": "GtGUAEVN_vni"
    },
    {
      "cell_type": "markdown",
      "id": "afe27e49-10c7-4c12-adea-48b0a05a5681",
      "metadata": {
        "id": "afe27e49-10c7-4c12-adea-48b0a05a5681"
      },
      "source": [
        "## 2.2 Discussion of results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ElMo didn't work at all, randomly suggesting the corresponding hypernyms. Fasttext showed very good results. "
      ],
      "metadata": {
        "id": "WAPf383eCH_k"
      },
      "id": "WAPf383eCH_k"
    },
    {
      "cell_type": "markdown",
      "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe",
      "metadata": {
        "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
      },
      "source": [
        "# 3. Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev5SFZNT-gCM"
      },
      "source": [
        "## Fasttext baseline"
      ],
      "id": "ev5SFZNT-gCM"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dialogue-evaluation/taxonomy-enrichment.git taxonomy_enrichment\n",
        "!cd taxonomy_enrichment/; git checkout 6ee12174163368f38276b6e58e81c21625660e13\n",
        "!mkdir ruwordnet && unzip taxonomy_enrichment/data/ruwordnet.zip -d ruwordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pn_86Z39pfZ",
        "outputId": "42989d31-d27f-435d-e88e-5ff0f7c07a23"
      },
      "id": "4Pn_86Z39pfZ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'taxonomy_enrichment'...\n",
            "remote: Enumerating objects: 204, done.\u001b[K\n",
            "remote: Total 204 (delta 0), reused 0 (delta 0), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (204/204), 19.11 MiB | 13.13 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n",
            "Note: checking out '6ee12174163368f38276b6e58e81c21625660e13'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 6ee1217 update link to the corpus\n",
            "Archive:  taxonomy_enrichment/data/ruwordnet.zip\n",
            "  inflating: ruwordnet/composed_of.xml  \n",
            "  inflating: ruwordnet/derived_from.xml  \n",
            "  inflating: ruwordnet/senses.A.xml  \n",
            "  inflating: ruwordnet/senses.N.xml  \n",
            "  inflating: ruwordnet/senses.V.xml  \n",
            "  inflating: ruwordnet/synsets.A.xml  \n",
            "  inflating: ruwordnet/synsets.N.xml  \n",
            "  inflating: ruwordnet/synsets.V.xml  \n",
            "  inflating: ruwordnet/synset_relations.A.xml  \n",
            "  inflating: ruwordnet/synset_relations.N.xml  \n",
            "  inflating: ruwordnet/synset_relations.V.xml  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('taxonomy_enrichment/baselines')"
      ],
      "metadata": {
        "id": "ZetxWlAGFo8a"
      },
      "id": "ZetxWlAGFo8a",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW73pDagEAyd",
        "outputId": "5cf08ee7-d949-4fb9-ceac-fd1d5da8b8f3"
      },
      "id": "GW73pDagEAyd",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p /content/drive_blue\n",
        "!google-drive-ocamlfuse /content/drive_blue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4wHR-ldF9Su",
        "outputId": "3bcf3c09-1c81-4055-eec9-58464492c1a6"
      },
      "id": "R4wHR-ldF9Su",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-gljjhEGD87",
        "outputId": "18489b4c-6e0d-442a-ba65-7972b7bc8a08"
      },
      "id": "L-gljjhEGD87",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 21 23:30:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.bin.gz\n",
        "!gunzip cc.ru.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzfHekHSGbzq",
        "outputId": "ae015c98-69c3-4af3-c782-5cee1e0927da"
      },
      "id": "DzfHekHSGbzq",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-21 23:30:47--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4496459151 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.ru.300.bin.gz’\n",
            "\n",
            "cc.ru.300.bin.gz    100%[===================>]   4.19G  12.1MB/s    in 5m 58s  \n",
            "\n",
            "2021-12-21 23:36:45 (12.0 MB/s) - ‘cc.ru.300.bin.gz’ saved [4496459151/4496459151]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xvIjg8kSzeN",
        "outputId": "0e794f5e-9fa9-47ec-adc0-1050dcba656b"
      },
      "id": "7xvIjg8kSzeN",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.1-py2.py3-none-any.whl (208 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3120201 sha256=601430624d0cb6458b4ebe06e0cb091dc9721386ce63c0b5627052e0f6fe3b7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import fasttext.util\n",
        "from string import punctuation\n",
        "from ruwordnet.ruwordnet_reader import RuWordnet\n",
        "\n",
        "\n",
        "class FasttextVectorizer:\n",
        "    def __init__(self, model_path):\n",
        "        self.model = fasttext.load_model(model_path)\n",
        "        print('Model loaded')\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # vectorize ruwordnet\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    def vectorize_ruwordnet(self, synsets, output_path):\n",
        "        ids, vectors = self.__get_ruwordnet_vectors(synsets)\n",
        "        self.save_as_w2v(ids, vectors, output_path)\n",
        "\n",
        "    def __get_ruwordnet_vectors(self, synsets):\n",
        "        ids = []\n",
        "        vectors = np.zeros((len(synsets), self.model.get_dimension()))\n",
        "        for i, (_id, texts) in enumerate(synsets.items()):\n",
        "            ids.append(_id)\n",
        "            vectors[i, :] = self.__get_avg_vector(texts)\n",
        "        return ids, vectors\n",
        "\n",
        "    def __get_avg_vector(self, texts):\n",
        "        sum_vector = np.zeros(self.model.get_dimension())\n",
        "        for text in texts:\n",
        "            words = [i.strip(punctuation) for i in text.split()]\n",
        "            sum_vector += np.sum(self.__get_data_vectors(words), axis=0)/len(words)\n",
        "        return sum_vector/len(texts)\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # vectorize data\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    def vectorize_data(self, data, output_path):\n",
        "        data_vectors = self.__get_data_vectors(data)\n",
        "        self.save_as_w2v(data, data_vectors, output_path)\n",
        "\n",
        "    def __get_data_vectors(self, data):\n",
        "        vectors = np.zeros((len(data), self.model.get_dimension()))\n",
        "        for i, word in enumerate(data):  # TODO: how to do it more effective or one-line\n",
        "            vectors[i, :] = self.model[word]\n",
        "        return vectors\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # save\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    @staticmethod\n",
        "    def save_as_w2v(words: list, vectors: np.array, output_path: str):\n",
        "        assert len(words) == len(vectors)\n",
        "        with open(output_path, 'w', encoding='utf-8') as w:\n",
        "            w.write(f\"{vectors.shape[0]} {vectors.shape[1]}\\n\")\n",
        "            for word, vector in zip(words, vectors):\n",
        "                vector_line = \" \".join(map(str, vector))\n",
        "                w.write(f\"{word.upper()} {vector_line}\\n\")\n",
        "\n",
        "\n",
        "def process_data(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        dataset = f.read().lower().split(\"\\n\")[:-1]\n",
        "    ft_vec.vectorize_data(dataset, output_file)"
      ],
      "metadata": {
        "id": "JxffgvKIHGJb"
      },
      "id": "JxffgvKIHGJb",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "from ruwordnet.database import DatabaseRuWordnet\n",
        "\n",
        "from taxonomy_enrichment.baselines.ruwordnet.ruwordnet_reader import *\n",
        "\n",
        "\n",
        "class RuWordnet(DatabaseRuWordnet):\n",
        "    def __init__(self, db_path, ruwordnet_path, with_lemmas=False):\n",
        "        super(RuWordnet, self).__init__(db_path)\n",
        "        self.with_lemmas = with_lemmas\n",
        "        self.__initialize_db(ruwordnet_path)\n",
        "\n",
        "    def __initialize_db(self, path):\n",
        "        if self.is_empty():\n",
        "            print(\"Inserting data to database\")\n",
        "            synset_files, relation_files, senses_files = get_wordnet_files_from_path(path)\n",
        "\n",
        "            synsets = [synset for file in synset_files for synset in parse_synsets(file)]\n",
        "            relations = [relation for file in relation_files for relation in parse_relations(file)]\n",
        "\n",
        "            if self.with_lemmas:\n",
        "                senses = [sense for file in synset_files for sense in parse_senses_lemmas(file)]\n",
        "            else:\n",
        "                senses = [sense for file in senses_files for sense in parse_senses(file)]\n",
        "\n",
        "            self.insert_synsets(synsets)\n",
        "            self.insert_relations(relations)\n",
        "            self.insert_senses(senses)"
      ],
      "metadata": {
        "id": "m2WGRSrpHeQc"
      },
      "id": "m2WGRSrpHeQc",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_vec = FasttextVectorizer(\"cc.ru.300.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkbbeV06Ip5w",
        "outputId": "5f809457-2824-4b35-d60f-db21e2c36796"
      },
      "id": "xkbbeV06Ip5w",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ruwordnet = RuWordnet(db_path=\"ruwordnet.db\", ruwordnet_path=\"ruwordnet\")"
      ],
      "metadata": {
        "id": "CYgm-l1gXbjj"
      },
      "id": "CYgm-l1gXbjj",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_synsets = defaultdict(list)\n",
        "verb_synsets = defaultdict(list)\n",
        "for sense_id, synset_id, text in ruwordnet.get_all_senses():\n",
        "    if synset_id.endswith(\"N\"):\n",
        "        noun_synsets[synset_id].append(text.lower())\n",
        "    elif synset_id.endswith(\"V\"):\n",
        "        verb_synsets[synset_id].append(text.lower())"
      ],
      "metadata": {
        "id": "hCXZWYQiYCj4"
      },
      "id": "hCXZWYQiYCj4",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_vec.vectorize_ruwordnet(noun_synsets, \"ruwordnet_nouns.txt\")\n",
        "ft_vec.vectorize_ruwordnet(verb_synsets, \"ruwordnet_verbs.txt\")"
      ],
      "metadata": {
        "id": "cEsJ_3FHYEzd"
      },
      "id": "cEsJ_3FHYEzd",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_data(\"taxonomy_enrichment/data/public_test/verbs_public.tsv\", \"verbs_public.txt\")\n",
        "process_data(\"taxonomy_enrichment/data/public_test/nouns_public.tsv\", \"nouns_public.txt\")\n",
        "process_data(\"taxonomy_enrichment/data/private_test/verbs_private.tsv\", \"verbs_private.txt\")\n",
        "process_data(\"taxonomy_enrichment/data/private_test/nouns_private.tsv\", \"nouns_private.txt\")"
      ],
      "metadata": {
        "id": "j5tRORuAYG91"
      },
      "id": "j5tRORuAYG91",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"ruwordnet_vectors_path\": \"ruwordnet_nouns.txt\",\n",
        "  \"data_vectors_path\": \"nouns_public.txt\",\n",
        "  \"test_path\": \"taxonomy_enrichment/data/public_test/nouns_public.tsv\",\n",
        "  \"output_path\": \"predicted_public_nouns_fasttext.tsv\",\n",
        "  \"db_path\": \"ruwordnet.db\",\n",
        "  \"ruwordnet_path\": None\n",
        "}\n",
        "\n",
        "from taxonomy_enrichment.baselines.main import *\n",
        "\n",
        "with open(params['test_path'], 'r', encoding='utf-8') as f:\n",
        "    test_data = f.read().split(\"\\n\")[:-1]\n",
        "  \n",
        "baseline = SecondOrderModel(params)\n",
        "results = baseline.predict_hypernyms(list(test_data))\n",
        "save_to_file(results, params['output_path'], baseline.ruwordnet)"
      ],
      "metadata": {
        "id": "fbbqePjiYOpz"
      },
      "id": "fbbqePjiYOpz",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"ruwordnet_vectors_path\": \"ruwordnet_verbs.txt\",\n",
        "  \"data_vectors_path\": \"verbs_public.txt\",\n",
        "  \"test_path\": \"taxonomy_enrichment/data/public_test/verbs_public.tsv\",\n",
        "  \"output_path\": \"predicted_public_verbs_fasttext.tsv\",\n",
        "  \"db_path\": \"ruwordnet.db\",\n",
        "  \"ruwordnet_path\": None\n",
        "}\n",
        "\n",
        "from taxonomy_enrichment.baselines.main import *\n",
        "\n",
        "with open(params['test_path'], 'r', encoding='utf-8') as f:\n",
        "    test_data = f.read().split(\"\\n\")[:-1]\n",
        "  \n",
        "baseline = SecondOrderModel(params)\n",
        "results = baseline.predict_hypernyms(list(test_data))\n",
        "save_to_file(results, params['output_path'], baseline.ruwordnet)"
      ],
      "metadata": {
        "id": "U8AkCiG-YSgg"
      },
      "id": "U8AkCiG-YSgg",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"ruwordnet_vectors_path\": \"ruwordnet_nouns.txt\",\n",
        "  \"data_vectors_path\": \"nouns_private.txt\",\n",
        "  \"test_path\": \"taxonomy_enrichment/data/private_test/nouns_private.tsv\",\n",
        "  \"output_path\": \"predicted_private_nouns_fasttext.tsv\",\n",
        "  \"db_path\": \"ruwordnet.db\",\n",
        "  \"ruwordnet_path\": None\n",
        "}\n",
        "\n",
        "from taxonomy_enrichment.baselines.main import *\n",
        "\n",
        "with open(params['test_path'], 'r', encoding='utf-8') as f:\n",
        "    test_data = f.read().split(\"\\n\")[:-1]\n",
        "  \n",
        "baseline = SecondOrderModel(params)\n",
        "results = baseline.predict_hypernyms(list(test_data))\n",
        "save_to_file(results, params['output_path'], baseline.ruwordnet)"
      ],
      "metadata": {
        "id": "Ry96gdTUYgf0"
      },
      "id": "Ry96gdTUYgf0",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"ruwordnet_vectors_path\": \"ruwordnet_verbs.txt\",\n",
        "  \"data_vectors_path\": \"verbs_private.txt\",\n",
        "  \"test_path\": \"taxonomy_enrichment/data/private_test/verbs_private.tsv\",\n",
        "  \"output_path\": \"predicted_private_verbs_fasttext.tsv\",\n",
        "  \"db_path\": \"ruwordnet.db\",\n",
        "  \"ruwordnet_path\": None\n",
        "}\n",
        "\n",
        "from taxonomy_enrichment.baselines.main import *\n",
        "\n",
        "with open(params['test_path'], 'r', encoding='utf-8') as f:\n",
        "    test_data = f.read().split(\"\\n\")[:-1]\n",
        "  \n",
        "baseline = SecondOrderModel(params)\n",
        "results = baseline.predict_hypernyms(list(test_data))\n",
        "save_to_file(results, params['output_path'], baseline.ruwordnet)"
      ],
      "metadata": {
        "id": "Gp6luC0DYrcw"
      },
      "id": "Gp6luC0DYrcw",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ELMo"
      ],
      "metadata": {
        "id": "txbnMs_F_Xm7"
      },
      "id": "txbnMs_F_Xm7"
    },
    {
      "cell_type": "code",
      "source": [
        "from ruwordnet.ruwordnet_reader import RuWordnet\n",
        "\n",
        "ruwordnet = RuWordnet(db_path='ruwordnet.db', ruwordnet_path=\"\")"
      ],
      "metadata": {
        "id": "OHjh_dSCYx00"
      },
      "id": "OHjh_dSCYx00",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simple_elmo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yDkecdLZGpw",
        "outputId": "fd3641bf-3c9a-43ca-f2bf-15b104b5407c"
      },
      "id": "4yDkecdLZGpw",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simple_elmo\n",
            "  Downloading simple_elmo-0.9.0-py3-none-any.whl (46 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████                         | 10 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 30 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 40 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 46 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (3.1.0)\n",
            "Requirement already satisfied: smart-open>1.8.1 in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (5.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simple_elmo) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->simple_elmo) (1.5.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simple_elmo) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simple_elmo) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->simple_elmo) (1.15.0)\n",
            "Installing collected packages: simple-elmo\n",
            "Successfully installed simple-elmo-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O tayga_lemmas_elmo_2048_2019.zip http://vectors.nlpl.eu/repository/20/199.zip\n",
        "!unzip tayga_lemmas_elmo_2048_2019.zip -d tayga_lemmas_elmo_2048_2019"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHZkMdpWZJO1",
        "outputId": "87523acc-496d-4808-a478-c583fdf03024"
      },
      "id": "qHZkMdpWZJO1",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-22 00:17:14--  http://vectors.nlpl.eu/repository/20/199.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1800442066 (1.7G) [application/zip]\n",
            "Saving to: ‘tayga_lemmas_elmo_2048_2019.zip’\n",
            "\n",
            "tayga_lemmas_elmo_2 100%[===================>]   1.68G  64.4MB/s    in 1m 59s  \n",
            "\n",
            "2021-12-22 00:19:13 (14.4 MB/s) - ‘tayga_lemmas_elmo_2048_2019.zip’ saved [1800442066/1800442066]\n",
            "\n",
            "Archive:  tayga_lemmas_elmo_2048_2019.zip\n",
            "  inflating: tayga_lemmas_elmo_2048_2019/meta.json  \n",
            "  inflating: tayga_lemmas_elmo_2048_2019/model.ckpt.data-00000-of-00001  \n",
            "  inflating: tayga_lemmas_elmo_2048_2019/model.ckpt.index  \n",
            "  inflating: tayga_lemmas_elmo_2048_2019/model.ckpt.meta  \n",
            "  inflating: tayga_lemmas_elmo_2048_2019/model.hdf5  \n",
            "  inflating: tayga_lemmas_elmo_2048_2019/options.json  \n",
            "  inflating: tayga_lemmas_elmo_2048_2019/README  \n",
            "  inflating: tayga_lemmas_elmo_2048_2019/vocab.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "def save_as_w2v(vectors_dict, output_path):\n",
        "    with open(output_path, 'w', encoding='utf-8') as w:\n",
        "        w.write(f\"{len(vectors_dict)} {len( vectors_dict[list(vectors_dict.keys())[0]] )}\\n\")\n",
        "        for key in vectors_dict:\n",
        "            vector_line = \" \".join(map(str, vectors_dict[key]))\n",
        "            w.write(f\"{key.upper()} {vector_line}\\n\")\n",
        "\n",
        "def elmo_vectorize(model, data, output_path):\n",
        "    batch_size = model.batch_size\n",
        "    result = defaultdict(list)\n",
        "\n",
        "    keys_b = []\n",
        "    value_b = []\n",
        "    for k in tqdm(data, position=0, leave=True):\n",
        "        keys_b.append(k)\n",
        "        value_b.append(data[k])\n",
        "        if len(keys_b) != batch_size:\n",
        "            continue\n",
        "\n",
        "        res = model.get_elmo_vector_average(value_b);\n",
        "        for i, ib in enumerate(keys_b):\n",
        "            result[ib] = res[i]\n",
        "        keys_b = []\n",
        "        value_b = []\n",
        "    \n",
        "    if keys_b:\n",
        "        res = model.get_elmo_vector_average(value_b);\n",
        "        for i, ib in enumerate(keys_b):\n",
        "            result[ib] = res[i]\n",
        "    \n",
        "    save_as_w2v(result, output_path)\n",
        "\n",
        "def process_data(model, input_file, output_file):\n",
        "    data = defaultdict(list)\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        dataset = f.read().lower().split(\"\\n\")[:-1]\n",
        "    for i in dataset:\n",
        "        data[i] = i\n",
        "    elmo_vectorize(model, data, output_file)"
      ],
      "metadata": {
        "id": "davzMjFsf814"
      },
      "id": "davzMjFsf814",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_elmo import ElmoModel\n",
        "\n",
        "model = ElmoModel()\n",
        "model.load('tayga_lemmas_elmo_2048_2019', max_batch_size=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "a2YQ6okLidF7",
        "outputId": "c32742b2-4ee5-42f9-ae41-e26199439ee2"
      },
      "id": "a2YQ6okLidF7",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-22 00:28:41,064 : INFO : Loading model from tayga_lemmas_elmo_2048_2019...\n",
            "2021-12-22 00:28:41,068 : INFO : We will cache the vocabulary of 100 tokens.\n",
            "/usr/local/lib/python3.7/dist-packages/simple_elmo/model.py:531: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell_clip=cell_clip, proj_clip=proj_clip)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:1013: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_proj_partitioner)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The model is now loaded.'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "model.logger.setLevel(logging.CRITICAL)"
      ],
      "metadata": {
        "id": "rP9Txc1fijA3"
      },
      "id": "rP9Txc1fijA3",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "noun_synsets = defaultdict(list)\n",
        "verb_synsets = defaultdict(list)\n",
        "for sense_id, synset_id, text in ruwordnet.get_all_senses():\n",
        "    if synset_id.endswith(\"N\"):\n",
        "        noun_synsets[synset_id].append(text.lower())\n",
        "    elif synset_id.endswith(\"V\"):\n",
        "        verb_synsets[synset_id].append(text.lower())"
      ],
      "metadata": {
        "id": "f1P7bx_timYf"
      },
      "id": "f1P7bx_timYf",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elmo_vectorize(model, noun_synsets, 'ruwordnet_nouns_elmo.txt')\n",
        "elmo_vectorize(model, verb_synsets, 'ruwordnet_verbs_elmo.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGL0jZcHioaZ",
        "outputId": "5f8f5a58-4825-4480-aac6-7669993f3cf6"
      },
      "id": "DGL0jZcHioaZ",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29296/29296 [17:31<00:00, 27.86it/s]\n",
            "100%|██████████| 7521/7521 [05:17<00:00, 23.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_data(model, \"taxonomy_enrichment/data/public_test/verbs_public.tsv\", \"verbs_public_elmo.txt\")\n",
        "process_data(model, \"taxonomy_enrichment/data/public_test/nouns_public.tsv\", \"nouns_public_elmo.txt\")\n",
        "process_data(model, \"taxonomy_enrichment/data/private_test/verbs_private.tsv\", \"verbs_private_elmo.txt\")\n",
        "process_data(model, \"taxonomy_enrichment/data/private_test/nouns_private.tsv\", \"nouns_private_elmo.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwpfMSYniq90",
        "outputId": "29132151-51d3-44cc-d168-af7a02865146"
      },
      "id": "EwpfMSYniq90",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 175/175 [00:03<00:00, 44.07it/s]\n",
            "100%|██████████| 762/762 [00:28<00:00, 27.03it/s]\n",
            "100%|██████████| 350/350 [00:11<00:00, 31.57it/s]\n",
            "100%|██████████| 1525/1525 [01:00<00:00, 25.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"ruwordnet_vectors_path\": \"ruwordnet_nouns_elmo.txt\",\n",
        "  \"data_vectors_path\": \"nouns_public_elmo.txt\",\n",
        "  \"test_path\": \"taxonomy_enrichment/data/public_test/nouns_public.tsv\",\n",
        "  \"output_path\": \"predicted_public_nouns_elmo.tsv\",\n",
        "  \"db_path\": \"ruwordnet.db\",\n",
        "  \"ruwordnet_path\": None\n",
        "}\n",
        "\n",
        "from taxonomy_enrichment.baselines.main import *\n",
        "\n",
        "with open(params['test_path'], 'r', encoding='utf-8') as f:\n",
        "    test_data = f.read().split(\"\\n\")[:-1]\n",
        "  \n",
        "baseline = SecondOrderModel(params)\n",
        "results = baseline.predict_hypernyms(list(test_data))\n",
        "save_to_file(results, params['output_path'], baseline.ruwordnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x7AjhqGorR-",
        "outputId": "d0f04100-bff4-4ab7-d2ce-71b12e0618af"
      },
      "id": "2x7AjhqGorR-",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-22 01:03:12,081 : INFO : loading projection weights from ruwordnet_nouns_elmo.txt\n",
            "2021-12-22 01:04:03,395 : INFO : loaded (29296, 1024) matrix from ruwordnet_nouns_elmo.txt\n",
            "2021-12-22 01:04:03,402 : INFO : loading projection weights from nouns_public_elmo.txt\n",
            "2021-12-22 01:04:04,659 : INFO : loaded (762, 1024) matrix from nouns_public_elmo.txt\n",
            "2021-12-22 01:04:04,661 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"ruwordnet_vectors_path\": \"ruwordnet_verbs_elmo.txt\",\n",
        "  \"data_vectors_path\": \"verbs_public_elmo.txt\",\n",
        "  \"test_path\": \"taxonomy_enrichment/data/public_test/verbs_public.tsv\",\n",
        "  \"output_path\": \"predicted_public_verbs_elmo.tsv\",\n",
        "  \"db_path\": \"ruwordnet.db\",\n",
        "  \"ruwordnet_path\": None\n",
        "}\n",
        "\n",
        "from taxonomy_enrichment.baselines.main import *\n",
        "\n",
        "with open(params['test_path'], 'r', encoding='utf-8') as f:\n",
        "    test_data = f.read().split(\"\\n\")[:-1]\n",
        "  \n",
        "baseline = SecondOrderModel(params)\n",
        "results = baseline.predict_hypernyms(list(test_data))\n",
        "save_to_file(results, params['output_path'], baseline.ruwordnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmSsI_hfqd-I",
        "outputId": "02bfde56-add3-4b77-fdb9-0e2c08519659"
      },
      "id": "FmSsI_hfqd-I",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-22 01:04:37,968 : INFO : loading projection weights from ruwordnet_verbs_elmo.txt\n",
            "2021-12-22 01:04:49,736 : INFO : loaded (7521, 1024) matrix from ruwordnet_verbs_elmo.txt\n",
            "2021-12-22 01:04:49,739 : INFO : loading projection weights from verbs_public_elmo.txt\n",
            "2021-12-22 01:04:50,022 : INFO : loaded (175, 1024) matrix from verbs_public_elmo.txt\n",
            "2021-12-22 01:04:50,026 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"ruwordnet_vectors_path\": \"ruwordnet_nouns_elmo.txt\",\n",
        "  \"data_vectors_path\": \"nouns_private_elmo.txt\",\n",
        "  \"test_path\": \"taxonomy_enrichment/data/private_test/nouns_private.tsv\",\n",
        "  \"output_path\": \"predicted_private_nouns_elmo.tsv\",\n",
        "  \"db_path\": \"ruwordnet.db\",\n",
        "  \"ruwordnet_path\": None\n",
        "}\n",
        "\n",
        "from taxonomy_enrichment.baselines.main import *\n",
        "\n",
        "with open(params['test_path'], 'r', encoding='utf-8') as f:\n",
        "    test_data = f.read().split(\"\\n\")[:-1]\n",
        "  \n",
        "baseline = SecondOrderModel(params)\n",
        "results = baseline.predict_hypernyms(list(test_data))\n",
        "save_to_file(results, params['output_path'], baseline.ruwordnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS5Yc4_2qzAa",
        "outputId": "b4b6d5cc-88b7-444b-cca0-5cb856a24da0"
      },
      "id": "HS5Yc4_2qzAa",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-22 01:04:50,639 : INFO : loading projection weights from ruwordnet_nouns_elmo.txt\n",
            "2021-12-22 01:05:39,821 : INFO : loaded (29296, 1024) matrix from ruwordnet_nouns_elmo.txt\n",
            "2021-12-22 01:05:39,825 : INFO : loading projection weights from nouns_private_elmo.txt\n",
            "2021-12-22 01:05:44,349 : INFO : loaded (1525, 1024) matrix from nouns_private_elmo.txt\n",
            "2021-12-22 01:05:44,356 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"ruwordnet_vectors_path\": \"ruwordnet_verbs_elmo.txt\",\n",
        "  \"data_vectors_path\": \"verbs_private_elmo.txt\",\n",
        "  \"test_path\": \"taxonomy_enrichment/data/private_test/verbs_private.tsv\",\n",
        "  \"output_path\": \"predicted_private_verbs_elmo.tsv\",\n",
        "  \"db_path\": \"ruwordnet.db\",\n",
        "  \"ruwordnet_path\": None\n",
        "}\n",
        "\n",
        "from taxonomy_enrichment.baselines.main import *\n",
        "\n",
        "with open(params['test_path'], 'r', encoding='utf-8') as f:\n",
        "    test_data = f.read().split(\"\\n\")[:-1]\n",
        "  \n",
        "baseline = SecondOrderModel(params)\n",
        "results = baseline.predict_hypernyms(list(test_data))\n",
        "save_to_file(results, params['output_path'], baseline.ruwordnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIQT6jiqq1aW",
        "outputId": "3d14c44d-adbb-44c4-97fe-fbf69523d2bf"
      },
      "id": "sIQT6jiqq1aW",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-22 01:06:11,028 : INFO : loading projection weights from ruwordnet_verbs_elmo.txt\n",
            "2021-12-22 01:06:21,752 : INFO : loaded (7521, 1024) matrix from ruwordnet_verbs_elmo.txt\n",
            "2021-12-22 01:06:21,754 : INFO : loading projection weights from verbs_private_elmo.txt\n",
            "2021-12-22 01:06:22,323 : INFO : loaded (350, 1024) matrix from verbs_private_elmo.txt\n",
            "2021-12-22 01:06:22,333 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XsPIOapFraIO"
      },
      "id": "XsPIOapFraIO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "HW3 - Kundyz Onlabek.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}